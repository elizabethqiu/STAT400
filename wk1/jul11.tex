\section{Monday, July 11, 2022}
This class is STAT400: Applied Probability and Statistics I. Topics covered: random variables, standard distributions, moments, law of large numbers and central limit theorem, sampling methods, estimation of parameters, testing of hypotheses.

\subsection{Logistics}
\begin{itemize}
	\item Textbook: Devore (2018), Probability and Statistics for Engineering and the Sciences ($9\th$ ed). Cengage Learning.
    \item All lectures are recorded and posted on Panopto.
    \item Frequent homework assignments and possibly pop quizzes. Assignments on ELMS.
    \item Assignments on ELMS. 
    \item The grade breakdown is $10\%$ for Homeworks, $10\%$ for Quizzes, $25\%$ for each Midterm (there are 2), and $40\%$ for the Final Exam. 
\end{itemize} 

\subsection{Probability, Statistics and Data}
In STAT400, we will visualizing and summarize data. At the end we will develop mathematical methods for data analysis.
\begin{itemize}
	\item \vocab{Statistics} is the science of recognizing, collecting, analyzing, interpreting data.
    \item \vocab{Probability} is a real-valued function of all events in a class of events such that unions, intersections and complements are also events. It is the mathematical theory of randomness and most of this course will be devoted to modeling random phenomena.
	\item \vocab{Randomness} almost always arises in real world data.
\end{itemize}

\textbf{Example 1}: We went over the "Newcomb's 1882 Measurements" example, where 66 measurements of the passage time of light were recorded by Newcomb in 1882 (values divided by 1000 plus 24 give the time in microseconds for light to traverse a known distance). We had a dataset with a summary of a minimum, 1st quartile value, median, mean, 3rd quartile value, maximum value, and standard deviation.

What do the summaries tell us?
\begin{itemize}
    \item The \vocab{mean} and \vocab{median} might both be considered as "typical" values of the data.
    \item The mean is \[\bar{X} = \frac{1}{66}\sum_{i=1}^{66}X_i,\] the \textit{sample average} of the data.
    \item The median $\bar{X}$ is the value such that half of the observations are larger than \bar{X} and half are smaller.
    \item We need to assess how much variation or \textit{spread} is contained in the data. The sample range (max-min) or interquartile range might be used, but a better choice is the standard deviation, defined by \[s^2 = \frac{1}{66-1}\sum_{i=1}^{66}(X_i-\bar{X})^2\].
\end{itemize}

Once we have the central values of the data, how far do we expect the values to be? \vocab{Standard deviation} is a value that tells you how far away (absolute value) from the typical observation we see data from the mean.

To visualize data, a \vocab{Histogram} is a very helpful shape of the graph that tells you typical distributions. \vocab{Outliers} are points outside of the bulk of data.

Interpreting histograms:
\begin{itemize}
    \item Values that are clearly separated from others are outliers.
    \item The histogram roughly follows the "bell curve", where there are few values at the low ends of scale, rising to a maximum near the mean or median, and then a decline from the maximum. Bell shapes are not easily recognized without visual aid the graph.
    \item Dropping outliers will only slightly change the mean and median, but may greatly affect the standard deviation, because outliers affect the spread.
    \item Histograms give good visualizations of shapes and distributions, which allow for comparing samples graphically.
\end{itemize}

\textbf{Example 2}: We also went over a Cereal example, using boxplots to compare calories from different Cereal brands. 

Another way to simplify the visualization of a dataset is with a \vocab{Boxplot}. It's made up of the minimum, 1st quartile, median, 3rd quartile, and maximum. The measure spread is the \vocab{interquartile range}.

The boxplot of a single sample is not really that useful, especially where calculations can be made instantly with a couple lines of code. But it's just as easy to create a histogram. And histograms are not a reliable indicator when you have small samples of data, like we have in the Newcomb's 1882 Measurements example.

And don't worry too much about Count Data; most data we will encounter in this course is Measurement Data.

We've seen unusual features of the previous two data sets which may be random or due to some systematic differences. Both systematic differences and unusual observations should be investigated. The summaries and graphs suggest features of data that may be interpreted rigorously by employing the mathematical theory of probability. We will see how to make reliable inferences about data sets like these, once we understand probability theory. That will be our next topic.

Recommended: We will use the \vocab{R} statistical package in this course. R is a general purpose system which has sophisticate mathematical and statistical functions as well as data handling routines. Learn R. It's also free! Book: Dalgard, P. (2008), \textit{Introductory Statistics with R, Springer.}

\subsection{Sample Spaces, Outcomes, and Events}
Most of this course will be based in probability. Probability theory is based on the concept of a \vocab{random experiment}. The outcome of the experiment is random, but the set of all possible outcomes is known.

We are usually not interested in single outcomes, but rather sets of outcomes, known as \vocab{events}. Events can be described using ``and" (\cap), ``or" (\cup), and ``not" $(\mathsf{'})$. We assign probabilities to these events.

The set of all outcomes is the \vocab{sample space}, denoted \textit{S}, and the individual outcomes are denoted \textit{s}. The sample space can also be continuous.

\textbf{Example 1}: tossing a coin n times, whether fair or unfair, will result in $2^n$ outcomes. The outcomes are H and T. For one coin toss, \textit{S} = \{H, T\}. For three consecutive coin tosses, \textit{S} = \{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT\}.

Probabilities are numerical values assigned to events. For example, in the experiment of tossing three coins, events of interest might be \textit{A} = \{More heads than tails\} or \textit{B} = \{First toss is T\}. Each event describes a set of outcomes. For example, \textit{A} = \{HHH, HHT, HTH, THH\}, \textit{B} = \{THH, THT, TTH, TTT\}.

To combine two events \textit{A} and \textit{B}, we write $\textit{A} \cap \textit{B}$ for ``\textit{A} and \textit{B}" and $\textit{A} \cup \textit{B}$ for ``A or B". Note that $\textit{A} \cup \textit{B}$ means that \textit{A} occurs or \textit{B} occurs or both occur. The complement of \textit{A}, or ``not \textit{A}", is denoted $A^{\mathsf{'}}$.

\subsection{Relative Frequency and Probability}
\begin{itemize}
    \item Probability is a numerical value attached to an event which may or may not occur in a random experiment.
    \item Baseball is a random experiment: a batter may or may not get a hit. A batter has hundreds of trials of this experiment. A given at-bat is unpredictable, but over a long season the batter's \textit{proportion} of hits becomes stable. This is his batting average. We assume the experiment was conducted more or less identically, and that the player is consistent.
    \item Consider a long sequence of trials of a random experiment, performed under identical conditions. After \textit{N} trials, the event \textit{A} occurs \textit{N(A)} times. The relative frequency of \textit{A}, $f(A) = \frac{N(A)}{N}$, tends toward a constant value \textit{P(A)}.
\end{itemize}

Properties of relative frequency:
\begin{itemize}
    \item $0\le\frac{N(A)}{N}\le1$
    \item $\frac{N(S)}{N}=1$. This means that \textit{S} is an event. So is $S^{\mathsf{'}}=\varnothing$.
    \item If \textit{A} and \textit{B} are \vocab{exclusive} or \vocab{disjoint}, meaning they can't occur at the same time, then $\frac{N(A\cup B)}{N}=\frac{N(A)}{N}+\frac{N(B)}{N}$.
\end{itemize}

These properties are obvious and should also hold for probabilities, which are limiting relative frequencies. As for implications of relative frequency for random samples, there is a shared characteristic that after an initial ragged, irregular, or unpredictable behavior over the first few trials, it tends to stabilize towards a value \textit{P(A)}. The randomness never goes away, but it does get averaged out.

Later, we can use data and real world random experiment observations to create mathematical models.

\subsection{Axioms of Probability}
Since probability is a mathematical concept, it must follow some mathematical rules. They must satisfy the following axioms:
\begin{enumerate}
    \item For any event \textit{A}, $O\le P(A)\le1$.
    \item The sample space \textit{S} satisfies $P(S)=1$, where \textit{S} resembles a finite or infinite sequence of outcomes.
    \item Let $A_{1}, A_{2}, \ldots, A_{n}$ be any sequence of disjoint events. Then \[A_{1}, A_{2}, \ldots = \sum_{n=1}^{\infty}P(A_{n})\].
\end{enumerate}

Axioms 1 and 2 resemble properties of relative frequency, but we regard them as defining a mathematical model.

Why we need Axiom 3: Suppose we have a long sequence of independent trials and cannot describe it the union of finitely many events. Therefore, we assume Axiom 3 to describe its probability; it's extended to handle infinite spaces.